import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
import warnings

# Suprimir avisos de deprecation para um output mais limpo
warnings.filterwarnings('ignore')

# --- 1. Carregar e Limpar Dados (Tratamento Inicial) ---
print("--- 1. Carregamento e Limpeza Inicial dos Dados ---")

# Carregar o dataset
try:
    df = pd.read_csv('breast-cancer.csv')
except FileNotFoundError:
    print("ERRO: O arquivo 'data.csv' não foi encontrado. Certifique-se de que está no diretório correto.")
    exit()

# 1.1. Tratar valores ausentes/colunas irrelevantes:
# - A coluna 'id' é apenas um identificador.
# - A coluna 'Unnamed: 32' está completamente vazia (valor ausente para todas as linhas).
#    OBS: A coluna 'Unnamed: 32' já foi verificado que não existe no dataset 'breast-cancer.csv'.
cols_to_drop = ['id'] # Removido 'Unnamed: 32' pois não está presente no dataset
df_clean = df.drop(columns=cols_to_drop)

print(f"Colunas removidas: {cols_to_drop}")
print(f"Shape do DataFrame após limpeza: {df_clean.shape}")

# --- 2. Conversão da Variável Alvo para Binário ---
print("\n--- 2. Conversão da Variável Alvo para Binário ---")

# 2.1. Converter 'diagnosis' para valores binários: M=1 (Maligno) e B=0 (Benigno)
df_clean['diagnosis_encoded'] = df_clean['diagnosis'].map({'M': 1, 'B': 0})

# 2.2. Definir Variáveis Preditoras (X) e Variável Alvo (Y)
# X: Todas as colunas estatísticas (Features)
# Y: Coluna 'diagnosis_encoded' (Alvo binário)
X = df_clean.drop(columns=['diagnosis', 'diagnosis_encoded'])
Y = df_clean['diagnosis_encoded']

print(f"Alvo 'diagnosis' mapeado para 1 (M) e 0 (B).")
print(f"Variável Alvo (Y) definida: {Y.name}")
print(f"Número de Variáveis Preditoras (X): {X.shape[1]}")


# --- 3. Separação de Dados em Treino e Teste (70/30) ---
print("\n--- 3. Separação de Dados em Treino e Teste (70/30) ---")

# Usando 70% para treino e 30% para teste (dentro da faixa 70-80% / 20-30%)
# random_state garante a reprodutibilidade da divisão
# stratify=Y garante que a proporção de M e B seja mantida nos conjuntos de treino e teste
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y,
    test_size=0.3, # 30% para teste
    random_state=42,
    stratify=Y # Estratificação pelo alvo
)

print(f"Divisão Treino/Teste (70%/30%):")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"Y_train shape: {Y_train.shape}")
print(f"Y_test shape: {Y_test.shape}")


# --- 4. Padronização dos Dados (StandardScaler) ---
print("\n--- 4. Padronização dos Dados (StandardScaler) ---")

# 4.1. Inicializar o StandardScaler
scaler = StandardScaler()

# 4.2. O StandardScaler deve ser TREINADO (fit) APENAS nos dados de TREINO.
# Isso evita o Vazamento de Dados (Data Leakage) do conjunto de teste.
X_train_scaled = scaler.fit_transform(X_train)

# 4.3. Aplicar a transformação (transform) nos conjuntos de TREINO e TESTE.
X_test_scaled = scaler.transform(X_test)

# O resultado da padronização é um array NumPy.
# Convertê-lo de volta para DataFrame para manter os nomes das colunas (opcional, mas útil para inspeção)
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)

print("StandardScaler aplicado:")
print("  - Ajustado (fit) apenas no X_train.")
print("  - Transformado em X_train e X_test.")
print("Média (deve ser próximo a 0) e Desvio Padrão (deve ser próximo a 1) para X_train_scaled:")
print(f"Média: {X_train_scaled.mean().mean():.4f}")
print(f"Desvio Padrão: {X_train_scaled.std().mean():.4f}")

print("\n--- Processamento Concluído ---")
print("Os datasets X_train_scaled, X_test_scaled, Y_train, e Y_test estão prontos para a Modelagem.")
